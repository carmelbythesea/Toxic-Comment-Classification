# Toxic-Comment-Classification

This project is designed to classify toxic comments from Wikipedia into 6 types: 'toxic', 'severe toxic', 'obscene', 'threat', 'insult', 'identity hate'. We used common classifiers Logistic regression, Isolation Forest and Multinomial Naive Bayes. We also used neural network CNN and LSTM to train the classification model.

Data are achived from the [Kaggle competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)

## Getting started

Download python 3 on the computer and download the code and packages listed below to your local machine to setup the model

### Prerequisites

- Python 
- Jupyter Notebook 
- Numpy Pandas 
- Matplotlib 
- Sklearn
- SciPy
- tensorflow 1.7.0rc1
- Keras 2.1.5
- gensim 3.4.0
- nltk 3.2.4
- wordcloud 1.4.1

### Installing

First to install Python 3 onto your computer 

Then to install all the packages needed to run the programs

`pip install numpy pandas matplotlib scikit-learn scipy`
`jupyter notebook` 


## Running tests

- For exploratory data analysis, run `eda_common_classifiers.ipynb`
- For common classifiers Logistic regression, Isolation Forest and Multinomial Naive Bayes, run `eda_common_classifiers.ipynb`
- For neural network CNN, run `cnn.ipynb`
- For neural network CNN+LSTM, run ``

## Authors

* **Qingyuan Pan** [Github](https://github.com/panqingyuan)
* **Zishun Jin** [Github](https://github.com/354352231)
* **Yingyin Xiao** [Github](https://github.com/carmelbythesea)

## References

- Jigsaw/Conversation AI. (Dec, 2017). Toxic Comment Classification Challenge. Retrieved February, 2022 from https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data
